"""
Scrapes disease outbreak data from text files generated by pdftotext on PDF files published by IDSP.NIC.IN. These are consolidated weekly reports on disease outbreaks across India.

Author: Angus Laurenson, Nov 2019

Usage: This script is intended to be run as command line tool, i.e.

>> python3 idsp_parser /path/to/txtfiles/*.txt

KNOWN ISSUES:

    Outbreaks not accurately found in pre 2016 files due to lack of consistent formatting. A rethink on the outbreak finding routine is necessary.

    Followup reporting. Don't know what the impact, meaning or solution to follow up reports.

    Wild card diseases which aren't on the initial list are not found

    Dates are spelt differently so some slip through the gaps. This gap has been closed most of the way.


Wishlist: pdftotext within this program,

Notes: SOPs for strings is capitalised words as in .title() string method. Lead taken from the datasets themselves.
"""

from fuzzywuzzy import fuzz
from datetime import datetime
import geopandas as gpd
import pandas as pd
import re
import sys
from tqdm import tqdm
import os

with open('err_log.txt','w') as f:
    f.write('# amateur log files for parsing errors')

# error tracking dict()
errors = {
    "state":0,
    "disease":0,
    "parsing_error":0,
    "unknown_year":0,
    'epoch_error':0
}

def fuzzy_match(hypotheses, target):
    # returns the hypothesis which best matches the target
    match = []

    # build a list of (score, state) tuples
    for h in hypotheses:
        match.append((fuzz.token_set_ratio(h,target), h))

    # sort the list of tuples to take the state with highest score
    match.sort()
    return match[-1][-1]

def outbreak_parser(outbreak):
    # issues with leaving types unconverted.

    # for validation, include raw outbreak line
    raw_string = outbreak

    # default values to account for missing values
    ID_code, state, district, disease, cases, deaths, start_date, report_date, status, comments = "?"*10

    # easy to locate fields
    ID_code = outbreak.split(" ")[0]
    try:
        status = re.findall("Under \w+",outbreak)
        comments = re.findall("((?<="+status+").*)", outbreak)
    except:
        pass

    # start stop dates
    dates = re.findall("(?<=\s)\d+\s?.?[/\.-][\s.]?\d+.?[/\.-]\d+", outbreak)

    try:
        # replace to help later to_datetime()
        start_date = dates[0].replace('/','-')
    except:
        pass
    try:
        report_date = dates[1]
    except:
        pass

    try:
        cases, deaths = re.findall("(?<=\s)\d+\*?[\s/\.-]\d+\*?(?=\s)",outbreak)[0].split(" ")
    except:
        pass

    # use a fuzzy match to robustly get state
    state = fuzzy_match(state_district_dict.keys(), outbreak)

    # search only the districts within the state
    district = fuzzy_match(state_district_dict[state], outbreak)

    # IDSP doctrine gives investigators freedom to record
    # diseases which they deem important. Therefore a
    # comprehensive list of diseases is impossible.
    # However we are interested in Cholera not rare wildcards

    for d in disease_names:
        if d in outbreak:
            disease = d
            break

    return [ID_code, state, district, disease, cases, deaths,  start_date, report_date, status, comments, raw_string]


def get_format(fname):
    # a function to obtain the epoch

    if 'pre_2016' in fname:
        return 'pre_2016'

    elif 'post_2016' in fname:
        return 'post_2016'

    else:
        return 1

def list_files(source):
    # get all txt files within source dir tree
    matches = []
    for root, dirnames, filenames in os.walk(source):
        for filename in filenames:
            if filename.endswith(('.txt')):
                matches.append(os.path.join(root, filename))
    return matches

def extract_post_2016_outbreaks(txt_file):
    # get a list of all outbreaks in text file
    # only for post 2016 data files

    # regex to detect ID code format for delimitng
    regex_post_2016 = "\w+/\w+/\d+/\d+/\d+"

    # open file and dump contents
    with open(txt_file,"r") as f:
        dump = f.read()
        dump = dump.replace("\n"," ")

    # list of outbreaks, delimited by the ID code regex
    outbreaks = re.findall(f"{regex_post_2016}.*?(?={regex_post_2016})",dump)

    return outbreaks

def extract_pre_2016_outbreaks(txt_file):
    # get a list of all outbreaks in text file
    # only for pre 2016 data files
    # bit ropey but should work

    # open the file and dump contents
    with open(txt_file) as f:
        dump = f.read()
        dump = dump.replace('\n',' ')

    # split in the centre as this is only reliable handle
    cases_deaths_date = re.findall('\d+\s\/?\s?\d+\s\d{2}.\d{2}.\d{2}',dump)
    dislocated_records = re.split('\d+\s\/?\s?\d+\s\d{2}.\d{2}.\d{2}',dump)

    outbreaks = []
    for i, record in enumerate(cases_deaths_date):

        # first 7 words left of the numerical data
        # should contain state district and disease
        front = ' '.join(dislocated_records[i].split(' ')[-7:])
        centre = record

        # just grab the whole next bit,
        # rear isn't so important
        rear = dislocated_records[i+1]

        outbreaks.append(' '.join((front,centre,rear)))

    return outbreaks

if __name__ == '__main__':
    # load a list of file names
    source = sys.argv[1]

    # walk the directory taking the txt txt_files
    text_files = list_files(source)
    print(text_files.__len__())

    # Build a list of outbreak strings
    outbreak_master = []

    # diagnostic count of pre pre_2016
    pre_2016_count = 0

    for text_file in text_files:

        epoch = get_format(text_file)

        if epoch == 'post_2016':
            outbreak_master += extract_post_2016_outbreaks(text_file)
        elif epoch == 'pre_2016':
            data = extract_pre_2016_outbreaks(text_file)
            pre_2016_count += len(data)
            outbreak_master += data
        else:
            errors['epoch_error'] += 1

    print("total number of outbreaks", outbreak_master.__len__())
    print("number of pre2016 outbreaks", pre_2016_count)
    print("number of post2016 outbreaks", outbreak_master.__len__()-pre_2016_count)

    # parse all the outbreaks strings
    # to create a useable dataframe

    IND_2 = gpd.read_file("gadm36_IND_2.shp")

    state_district_dict = {}
    for d in IND_2[['NAME_1','NAME_2']].values:
        # built a dictionary of lists
        # keys are states, values are its districts
        try:
            state_district_dict[d[0]].append(d[-1])
        except:
            state_district_dict[d[0]] = [d[-1]]

    # load list of diseases, not comprehensive
    with open("disease_names.txt","r") as f:
        disease_names = f.read().split("\n")

    # Create a dataframe which contains all the outbreaks as rows
    # and all the data fields as columns
    outbreaks = pd.DataFrame(columns = ["ID_code", "state", "district", "disease", "cases", "deaths", "start_date", "report_date", "status", "comments", "raw"])

    for i, raw in enumerate(tqdm(outbreak_master)):
        # this part accumulates a significant number of errors
        # I think outbreak_parser is failing?
        outbreaks.loc[i] = outbreak_parser(raw)

        # try:
        #     outbreaks.loc[i] = outbreak_parser(raw)
        # except:
        #     # print(outbreak_parser(raw))
        #     errors['parsing_error'] += 1
        #     with open('err_log.txt','r+') as err_log:
        #         err_log.write(' '.join(['parse_failure',str(i),raw,'\n']))
        # finally:
        #     pass

    # report the number of reports that we failed to read
    for key in errors.keys():
        print("number of {} errors = {}".format(key,errors[key]))

    # write out the dataframe to csv file for later analysis
    outbreaks.to_csv(r'IDSP_data.csv')
